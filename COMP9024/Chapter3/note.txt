"""
Created on Sun Jul 14 08:53:08 2019

@author: Michael
"""

"""
Chapter 3: Algorithm Analysis
"""

"""
3.1 Experimental Studies
study the running time after an algorithm implemented
time function from time module:
    from time import time
    start_time = time()
    run algorithm
    end_time = time()
    elapsed = end_time - start_time
clock function, timeit module

Visualisation method
plotting the performance of each run of the algorithm:
    x coordinates: input size n
    y coordinates: running time t

limitations:
    the comparison of two algorithms:
        same hardware and software environments
    limited size of inputs
    the algorithm should be fully implemented

3.1.1 Moving Beyond Experimental Analysis
goals:
    1 to evaluate the efficiency of algorithms independent of the hardware and software
      environment
    2 to perform by studying a high-level description of the algorithm without implementation
    3 all possible inputs

Counting Primitive Operations
* assigning an identifier to an object
* Determining the object associated with an identifier
* Performing an arithmetic operation
* comparing two numbers
* indexing
* calling a function
* Returning from a function

Measuring Operations as a Function of Input Size
the number of primitive operations can be performed as a function of the input size n

Focusing on the Worst-case Input
* wish to express the running time of an algorithm as the function of the input size
  obtained by taking the average over all possible inputs of the same size.
* average-case is challenging
* depending on the input distribution, running time can be anywhere between the worst-
  case and best-case.
* As to expect the running time to involve sophisticated ones, characterise the worst-case
* worst-case running time analysis is easier.
"""

"""
3.2 The Seven Functions Used in This Book

(1) Constant Function  f(n) = c
* for any argument n, the function will always return the value c
* the most fundamental constant function is g(n) = 1
* any other constant function f(n) = c*g(n)

(2) Logarithm Function f(n) = logb(n), b > 1
* b is base
* the most common base is 2
* logb(n) can be easily calculated by dividing n by b before a number less than 1 is obtained
* proposition (Logarithm rules):
    logb(ac) = logb(a) + logb(c)
    logb(a/c) = logb(a) - logb(c)
    logb(a^c) = c*logb(a)
    logb(a) = logd(a) / logd(b)
    b*logd(a) = a^logd(b)

(3) Linear Function f(n) = n
* for given input n, the linear function f assigns the value n itself
* when have to do a single basic operation for each of n elements
* represents the best running time expected to achiveve for any algorithm where each of
  n objects

(4) N-log-N function f(n) = n * log n
* This function grows a little more rapidly than the linear function
* a lot less rapidly than the quadratic function

(5) Quadratic Function f(n) = n^2
* algorithms with nested loops
* Nested loops and the Quadratic Function （f(n) = 1 + 2 + 3 + ... + n）

(6) Cubic Function and Other Polynomials
* Cubit Function f(n) = n^3
* Polynomial Function f(n) = a0 + a1*n + a2*n^2 + ... + ad*n^d
* Running time that are polynomials with small degree are generally better than polynomial
  running times with larger degree.

（7） Exponential Function f(n) = b^n
* exponent rules
    (b^a)^c = b^(a*c)
    b^a * b^c = b^(a+c)
    b^a / b^c = b^(a-c)
* b^(1/k) can be kth root of b, the number r such that r^k = b
* b^(a/c) where a/c can be a fraction, and the result can be the cth root of b^a
* b^(-d) = 1/b^d
* 1 + a + a^2 + ... + a^n = (a^(n+1) - 1) / (a - 1)
* when a = 2, 1 + 2 + 4 + ... + 2^(n-1) = 2^n - 1
"""
"""
3.2.1 Comparing Growth rates
Constant < Logarithm < Linear < n-log-n  < Quadratic < Cubic < Exponential
    1    <   log n   <    n   < n*log(n) <    n^2    < n^3   <  a^n

Ceiling and Floor Functions
Floor: the largest integer less than x
Ceiling: the smallest integer greater than x
"""
